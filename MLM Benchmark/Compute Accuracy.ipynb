{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ja-en\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ja-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\Otaku-Benchmark\\Visual Novel\\japaneseOriginal.txt\", 'r', encoding = 'UTF-8') as inp:\n",
    "    Visual_Novel = [line.strip() for line in inp]\n",
    "with open(\".\\Otaku-Benchmark\\Light Novel\\japaneseOriginal.txt\", 'r', encoding = 'UTF-8') as inp:\n",
    "    Light_Novel = [line.strip() for line in inp]\n",
    "with open(\".\\Otaku-Benchmark\\Manga\\japaneseOriginal.txt\", 'r', encoding = 'UTF-8') as inp:\n",
    "    Manga = [line.strip() for line in inp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "   1. Add [CLS], [SEP] tokens\n",
    "   2. Use given tokenizer to tokenize the data in Otaku-Benchmark \n",
    "2. Metric_specific preprocessing:\n",
    "   1. mask fill task (original):\n",
    "      1. pick 20 nouns that have highest frequency from each domain\n",
    "      2. pick 10 sentences that contain them and mask the nouns\n",
    "      3. use these masked nouns as label\n",
    "3. Run the model and get output: (output should include embedding for [CLS] token, predicted masked words)\n",
    "   1. sentence_clustering task:\n",
    "   2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. mask fill task (original):\n",
    "      1. pick 20 nouns that have highest frequency from each domain\n",
    "      2. pick 10 sentences that contain them and mask the nouns\n",
    "      3. use these masked nouns as label\n",
    "2. sentence clustering task\n",
    "3. PPL\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_predict_accuracy(predicted_token, original_token):\n",
    "    return np.sum(predicted_token == original_token)/len(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\".\\Otaku-Benchmark\\Visual Novel\\japaneseOriginal.txt\", \".\\Otaku-Benchmark\\Light Novel\\japaneseOriginal.txt\", \".\\Otaku-Benchmark\\Manga\\japaneseOriginal.txt\"]\n",
    "tokenizer.train(files=paths, vocab_size=30_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=[\n",
    "                    '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"[CLS]\" + Visual_Novel[0] + \"[MASK]\" + \"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 1710, 720, 599, 32, 239, 1172, 14, 4, 3],\n",
       " ['[CLS]', '──', '空', '気', 'か', '動', 'いた', '。', '[MASK]', '[SEP]'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids, output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c26c644243da0b173a9fac6b3d9a7a203ca1eae3ac799a995e4a9b3c05675563"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('cs182_hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
